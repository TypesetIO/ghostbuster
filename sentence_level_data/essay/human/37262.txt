Americans, most of whom are Christians, believe that God is responsible for the country.