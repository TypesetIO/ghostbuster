The film industry has traditionally been male-dominated, with women often marginalized in terms of roles, representation, and opportunities.